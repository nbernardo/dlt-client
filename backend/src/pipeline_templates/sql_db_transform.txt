%metadata_section%import dlt
import polars as pl
from sqlalchemy import create_engine
from pathlib import Path
from sys import path
%import_from_src%
from src.utils.pipeline.PipelinesHelper import PipelineLogger, parse_aggregation

# Initialize pipeline logger
logger = PipelineLogger()

logger.info("Starting SQL database transformation pipeline", extra={'stage': 'initialization', 'template_type': 'sql_db_transform'})

#Adding root folder to allow import  from src
src_path = str(Path(__file__).parent).replace('/destinations/pipeline/%User_folder%','')
path.insert(0, src_path)
path.insert(0, src_path+'/src')

from src.services.workspace.SecretManager import SecretManager
from src.services.workspace.Workspace import Workspace

# Bellow mapping: namespace = SqlDBComponent.namespace
namespace = %namespace%

# Bellow mapping: connection_name = SqlDBComponent.connection_name
connection_name = %connection_name%

print(f'fetching "{connection_name}" secrets for DB connection', flush=True)
logger.info("Fetching database connection secrets", extra={'stage': 'authentication', 'namespace': namespace, 'connection_name': connection_name})
secret = SecretManager.get_db_secret_from_ppline(namespace, connection_name)

connection_string = secret['connection_url']

# SQLEngine for Polars .read_database()
logger.info("Creating database engine", extra={'stage': 'database_connection'})
engine = create_engine(connection_string)

# Bellow mapping: tables = SqlDBComponent.source_tables
tables = %source_tables%

# Bellow mapping: tables_pk = SqlDBComponent.primary_keys
tables_pk = %primary_keys%

# Bellow mapping: schemas = SqlDBComponent.schemas
schemas = %schemas%

# Bellow mapping is parse on transformation 
# controller.pipeline.template_final_parsing
transformations = %transformation%
%transformation2%

@dlt.source
def polars_db_source(tables: list[str], tables_pk: list[str]):

    def generate_resource(table_name, pk):

        @dlt.resource(name=table_name)
        def load_table():
            logger.info("Starting data extraction", extra={
                'stage': 'data_extraction',
                'table_name': table_name,
                'primary_key': pk
            })

            total_records = 0
            for df in pl.read_database(
                query=f"SELECT * FROM {table_name}",
                connection=engine,
                iter_batches=True,
                batch_size=10_000,
            ):
                batch_size = len(df)
                total_records += batch_size
                df = parse_aggregation(df, transformations.get(table_name, []))
                
                print(f'fetch data chunk from {table_name}', flush=True)
                logger.debug("Processing data batch", extra={
                    'stage': 'data_processing',
                    'table_name': table_name,
                    'batch_size': batch_size,
                    'total_records_so_far': total_records
                })

                # Apply table-specific transformations
                if transformations.get(table_name, None) != None:
                    logger.debug("Applying primary transformations", extra={
                        'stage': 'transformation_primary',
                        'table_name': table_name
                    })
                    # Remove any trasformation of type aggregations
                    columns = [item for item in transformations[table_name] if type(item) != dict]
                    df = df.with_columns(columns)
                    print(f'applied transformation for {table_name}', flush=True)
                    
                # Transformation with not column format change (e.g. dedup, column drop)
                if table_name in transformations2:
                    logger.debug("Applying secondary transformations", extra={
                        'stage': 'transformation_secondary',
                        'table_name': table_name,
                        'transformation_count': len(transformations2[table_name])
                    })
                    for transform in transformations2[table_name]: df = transform(df)

                if df.schema[pk] in [pl.Float32, pl.Float64]:
                    logger.debug("Converting primary key to string", extra={
                        'stage': 'data_type_conversion',
                        'table_name': table_name,
                        'primary_key': pk
                    })
                    df = df.with_columns([pl.col(pk).cast(pl.Utf8)])

                for row in df.iter_rows(named=True): yield row
            
            logger.info("Data extraction completed", extra={
                'stage': 'extraction_complete',
                'table_name': table_name,
                'total_records': total_records
            })

        return load_table.apply_hints(primary_key=pk)

    print(f'fetching data from source tables', flush=True)
    logger.info("Setting up data sources", extra={'stage': 'data_source_setup', 'table_count': len(tables)})
    return [generate_resource(table_name, pk) for table_name, pk in zip(tables, tables_pk)]

dest_folder = Workspace.get_duckdb_path_on_ppline()

%destination_settings%

source = polars_db_source(tables, tables_pk)

print(f'Running the pipeline', flush=True)
logger.info("Starting pipeline execution", extra={'stage': 'pipeline_execution', 'table_count': len(tables)})

try:
    load_info = pipeline.run(source)
    
    # Log pipeline execution results
    if hasattr(load_info, 'loads_ids') and load_info.loads_ids:
        logger.info("SQL database transformation pipeline execution completed successfully", extra={
            'stage': 'pipeline_completion',
            'loads_ids': str(load_info.loads_ids),
            'dataset_name': getattr(load_info, 'dataset_name', 'unknown'),
            'table_count': len(tables),
            'template_type': 'sql_db_transform'
        })
    else:
        logger.info("SQL database transformation pipeline execution completed", extra={
            'stage': 'pipeline_completion',
            'table_count': len(tables),
            'template_type': 'sql_db_transform'
        })

    print(load_info, flush=True)
    print('RUN_SUCCESSFULLY', flush=True)
    
except Exception as e:
    logger.error("SQL database transformation pipeline execution failed", extra={
        'stage': 'pipeline_error',
        'error_type': type(e).__name__,
        'error_message': str(e),
        'table_count': len(tables),
        'template_type': 'sql_db_transform'
    }, exc_info=True)
    print(f'RUNTIME_ERROR:SQL database transformation pipeline execution failed: {e}', flush=True)
    raise