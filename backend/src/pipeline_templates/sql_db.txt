%metadata_section%import dlt
from dlt.sources.credentials import ConnectionStringCredentials
from dlt.sources.sql_database import sql_database, sql_table
from os import getenv as env
from pathlib import Path
import sys

#Adding root folder to allow import  from src
src_path = str(Path(__file__).parent).replace('/destinations/pipeline/%User_folder%','')
sys.path.insert(0, src_path)
sys.path.insert(0, src_path+'/src')

from src.services.workspace.Workspace import Workspace
from src.services.workspace.SecretManager import SecretManager
from src.utils.SQLDatabase import normalize_table_names

# Bellow mapping: namespace = SqlDBComponent.namespace
namespace = %namespace%

# Bellow mapping: connection_name = SqlDBComponent.connection_name
connection_name = %connection_name%

print('connecting to secrets vault', flush=True)
SecretManager.ppline_connect_to_vault()

print(f'fetching "{connection_name}" secrets for DB connection', flush=True)
secret = SecretManager.get_db_secret(namespace, connection_name)

connection_string = secret['connection_url']

# Bellow mapping: source_database = SqlDBComponent.source_database
database_to_connect = %source_database%

credentials = ConnectionStringCredentials(connection_string)

def load_select_tables_from_db():
    dest_folder = Workspace.get_duckdb_path_on_ppline()
    ppline_name, output_name = %pipeline_name%, %output_dest_name%
    %dest_secret_code%
    dest = %destination_string%
    # Bellow mapping: output_dest_name = DuckDBOutput.output_dest_name
    pipeline = dlt.pipeline(pipeline_name=ppline_name,destination=dest,dataset_name=output_name)

    # Bellow mapping: tables = SqlDBComponent.source_tables
    tables = %source_tables%

    # Bellow mapping: tables_pk = SqlDBComponent.primary_keys
    tables_pk = %primary_keys%
    tables = normalize_table_names(secret, tables)

    # Bellow mapping: schema = SqlDBComponent.schema
    schema = %schema%

    if schema:
        source = []
        for idx in range(len(tables)):
            schema_name, table_name = tables[idx].split('.')
            db_table = sql_table(
                table=table_name,  credentials=credentials, schema=schema_name
            ).apply_hints(
                primary_key=tables_pk[idx]
            )

            source.append(db_table)

    else:
        source = sql_database(table_names=tables, credentials=credentials)
        for idx in range(len(tables)):
            getattr(source, tables[idx]).apply_hints(primary_key=tables_pk[idx])

    print('Starting pipeline run', flush=True)
    info = pipeline.run(source, write_disposition='merge')
    print(info, flush=True)

load_select_tables_from_db()