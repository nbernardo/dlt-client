%metadata_section%import dlt
from dlt.sources.credentials import ConnectionStringCredentials
from dlt.sources.sql_database import sql_database, sql_table
import logging
from os import getenv as env
from pathlib import Path
from sys import path

# Initialize pipeline logger
logger = logging.getLogger('pipeline.execution')
logger.info("Starting SQL database (old) pipeline", extra={'stage': 'initialization', 'template_type': 'sql_db_old'})

# Configure dlt internal logging
logger = logging.getLogger('dlt')
logger.setLevel(logging.DEBUG)
# Add handler to output logs
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(handler)

#Adding root folder to allow import  from src
src_path = str(Path(__file__).parent).replace('/destinations/pipeline/%User_folder%','')
path.insert(0, src_path)
path.insert(0, src_path+'/src')

from src.services.workspace.Workspace import Workspace

mysql_srv = env('MYSQLDBSRV')
mysql_usr = env('MYSQLDBUSR')
mysql_pwd = env('MYSQLDBPWD')
mysql_prt = env('MYSQLDBPRT')

# Bellow mapping: schema = SqlDBComponent.source_database
database_to_connect = %source_database%

dialects = {
    "mysql": f"mysql+pymysql://{mysql_usr}:\
            {mysql_pwd}@{mysql_srv}:\
            {mysql_prt}/{database_to_connect}" 
}

# Bellow mapping: schema = SqlDBComponent.source_dbengine
credentials = ConnectionStringCredentials(dialects[%source_dbengine%])

def load_select_tables_from_db():
    logger.info("Setting up database connection", extra={'stage': 'database_connection'})
    
    dest_folder = Workspace.get_duckdb_path_on_ppline()
    ppline_name, output_name = %pipeline_name%, %output_dest_name%
    %dest_secret_code%
    dest = %destination_string%
    # Bellow mapping: schema = DuckDBOutput.output_dest_name
    pipeline = dlt.pipeline(pipeline_name=ppline_name, destination=dest, dataset_name=output_name)

    # Bellow mapping: schema = SqlDBComponent.source_tables
    tables = %source_tables%

    # Bellow mapping: schema = SqlDBComponent.primary_keys
    tables_pk = %primary_keys%

    logger.info("Creating SQL database source", extra={'stage': 'data_source_setup', 'table_count': len(tables)})
    source = sql_database(table_names=tables, credentials=credentials)

    for idx in range(len(tables)):
        logger.debug("Applying primary key hints", extra={
            'stage': 'table_processing',
            'table_name': tables[idx],
            'primary_key': tables_pk[idx]
        })
        getattr(source, tables[idx]).apply_hints(primary_key=tables_pk[idx])

    logger.info("Starting pipeline execution", extra={'stage': 'pipeline_execution'})
    info = pipeline.run(source)
    
    # Log pipeline execution results
    if hasattr(info, 'loads_ids') and info.loads_ids:
        logger.info("SQL database (old) pipeline execution completed successfully", extra={
            'stage': 'pipeline_completion',
            'loads_ids': str(info.loads_ids),
            'dataset_name': getattr(info, 'dataset_name', 'unknown'),
            'template_type': 'sql_db_old'
        })
    else:
        logger.info("SQL database (old) pipeline execution completed", extra={'stage': 'pipeline_completion', 'template_type': 'sql_db_old'})
    
    print(info)
    print('RUN_SUCCESSFULLY', flush=True)

try:
    load_select_tables_from_db()
except Exception as e:
    logger.error("SQL database (old) pipeline execution failed", extra={
        'stage': 'pipeline_error',
        'error_type': type(e).__name__,
        'error_message': str(e),
        'template_type': 'sql_db_old'
    }, exc_info=True)
    print(f'RUNTIME_ERROR:SQL database (old) pipeline execution failed: {e}', flush=True)
    raise
