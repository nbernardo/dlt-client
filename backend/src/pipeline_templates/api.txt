%metadata_section%import dlt
from dlt.sources.helpers.rest_client import RESTClient
import requests
from pathlib import Path
import sys
%auth_strategy%
#Adding root folder to allow import from src
src_path = str(Path(__file__).parent).replace('/destinations/pipeline/%User_folder%','')
sys.path.insert(0, src_path)
sys.path.insert(0, src_path+'/src')

from src.services.workspace.Workspace import Workspace
from src.services.workspace.SecretManager import SecretManager
from src.utils.APIClientUtil import PaginateParam

# Bellow mapping: connection_name = InputAPI.connection_name
connection_name = %connection_name%
print('connecting to secrets vault', flush=True)
SecretManager.ppline_connect_to_vault()
# Bellow mapping: namespace = InputAPI.namespace
namespace = %namespace%
print(f'fetching "{connection_name}" secrets for DB connection', flush=True)
secret = SecretManager.get_secret(namespace, connection_name)

resource_names = %resource_names%
data_selectors = %data_selectors%
primary_keys = %primary_keys%
paginate_params = %paginate_params%
endpoints_params = %endpoints_params%

api_client = RESTClient(
    base_url=%base_url%,%auth_config%
)

@dlt.source
def get_api_data():

    def generate_source(
            path: str, 
            data_selector: str = None, 
            primary_key: str = None,
            page_param: PaginateParam = None,
            path_params = {}
        ):

        resource_name = path.split('/')[1]
        primary_key = primary_key if not ['',None].__contains__(primary_key) else None

        @dlt.resource(name=resource_name, primary_key=primary_key)
        def fetch_api_data(path, data_selector, page: PaginateParam = None):

            # Stating start and end record fetch is paginated
            start, end = None, None

            if page != None:
                start = page.start_record
                end = page.start_record + page.batch_size

            try:
                total_iterations = 0
                # While true takes place especially because of pagination possibility
                while True:
                    data_fetch = False
                    full_path = path if page == None else f'{path}?{page.start_param}={start}&{page.end_param}={end}'
                    data = api_client.get(full_path, path_params)
                    data_fetch = True
                    total_iterations = total_iterations + 1
                    if data_selector:
                        result = data.json().get(data_selector,[])
                    else:
                        result = data.json()
                    
                    if not result: break

                    yield result

                    # In no pagination needs to happe it forces to end the loop
                    if page == None: break
                    
                    start += page.batch_size
                    end = start + (page.batch_size - 1)
                    print(f'Fetched data from {full_path} endpoint', flush=True)
                
                print(f'Completed data fetch from {path} endpoint', flush=True)

            except requests.exceptions.RequestException as err:
                if data_fetch:
                    if total_iterations > 0:
                        print(f'RUNTIME_WARNING:No result found for {full_path}', flush=True)
                    else:
                        print(f'RUNTIME_ERROR:No result found for {full_path}', flush=True)
                else:
                    print('RUNTIME_ERROR:Error while fetching API data for resource: ', resource_name, flush=True)

        return fetch_api_data(path, data_selector, page_param)
    
    params_tuple = zip(resource_names, endpoints_params, primary_keys, data_selectors, paginate_params)
    for path, path_params, pk , selector, page_params in params_tuple:
        yield generate_source(path, selector, pk, page_params, path_params)


dest_folder = Workspace.get_duckdb_path_on_ppline()

%destination_settings%

load_info = pipeline.run(get_api_data())
print(load_info, flush=True)
print('RUN_SUCCESSFULLY', flush=True)